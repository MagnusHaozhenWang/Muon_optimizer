# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a PyTorch research project for comparing three optimizers (SGD, AdamW, and Muon) on the LeNet architecture using CIFAR-10 dataset. The project implements a custom Muon optimizer that uses Newton-Schulz iterations for gradient orthogonalization.

## Running Experiments

### Train all optimizers with all seeds (3×3 = 9 runs)
```bash
python main.py
```

### Run a specific optimizer
```bash
python main.py --optimizer Muon
```

### Run with specific seed and epoch override
```bash
python main.py --optimizer SGD --seed 42 --epochs 50
```

### Visualize results after training
```bash
python visualize.py
```

**Note:** Results are only generated after experiments complete. The `visualize.py` script reads JSON metrics from `logs/` directory.

## Architecture

### Module Structure

- **main.py** - Experiment orchestrator that runs all optimizer×seed combinations and aggregates results
- **config.py** - Centralized configuration using dataclasses (TrainConfig, SGDConfig, AdamWConfig, MuonConfig)
- **model.py** - LeNet architecture adapted for CIFAR-10 (32×32 RGB, not the original 28×28 MNIST)
- **optimizers.py** - Factory pattern for creating optimizers; imports the custom Muon class
- **muon.py** - Custom optimizer implementation using Newton-Schulz orthogonalization for 2D tensors
- **trainer.py** - Training loop with metrics collection, checkpointing, GPU/memory monitoring
- **dataset.py** - CIFAR-10 loading with fixed train/val split (45K/5K from training set)
- **utils.py** - Random seed setting, GPU monitoring (pynvml), step timing, memory stats
- **visualize.py** - Generate accuracy/loss curves, time/memory comparisons, and Markdown report

### Key Design Patterns

1. **Configuration via dataclasses** - All hyperparameters centralized in `config.py`. Modify optimizer configs via `get_optimizer_configs()` return dict.

2. **Deterministic training** - Seeds fixed for Python, NumPy, PyTorch (CPU+GPU), DataLoader workers. Data split uses fixed seed (`data_split_seed=42`).

3. **Metrics collection** - `Trainer` class outputs both CSV logs (per-epoch) and JSON metrics (full run summary). Final summary aggregates across seeds with mean±std.

4. **Two evaluation protocols** - Both "Last" (final epoch) and "Best-Val" (checkpoint with highest validation accuracy) are evaluated on test set.

### Muon Optimizer Implementation

The custom Muon optimizer (`muon.py`) applies Newton-Schulz iterations to orthogonalize gradients for 2D parameters (weight matrices in Linear/Conv layers), while using standard momentum updates. Key details:
- Converts gradients to bfloat16 during orthogonalization
- Only applies to 2D tensors (gradients with `ndim == 2`)
- Uses decoupled weight decay (like AdamW)
- Supports Nesterov momentum

### Output Directories

- `logs/` - CSV logs and JSON metrics per run
- `checkpoints/` - Model checkpoints (`_last.pth` and `_best.pth`)
- `results/` - Summary JSON, plots, and Markdown report (generated by `visualize.py`)
- `data/` - CIFAR-10 dataset download location

## Dependencies

Standard PyTorch stack:
- `torch`, `torchvision`
- `numpy`
- `matplotlib`
- `pynvml` (optional - for GPU utilization monitoring; gracefully degrades if unavailable)

No package.json, requirements.txt, or setup.py exists - install dependencies manually as needed.
